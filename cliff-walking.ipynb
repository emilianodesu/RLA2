{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Q-Learning\n",
    "\n",
    "This notebook will explore the implementation of a Temporal-Difference (TD) method, Q-Learning, on the CliffWalking environment.\n",
    "\n",
    "* Q-Learning Algorithm for Gym based environments\n",
    "\n",
    "### What you will learn?\n",
    "* Implement Q-Learning Algorithm\n",
    "* Use Q-Learning on Cliffwalking and Environments"
   ],
   "id": "42eea99995e09460"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Imports and Utilities",
   "id": "febde2c365f36c31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install gymnasium pyvirtualdisplay\n",
    "!apt-get install -y xvfb ffmpeg\n",
    "!pip install moviepy"
   ],
   "id": "dac96e5d8ea42063"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])"
   ],
   "id": "852b7a65f6a8502f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Utility functions",
   "id": "e7a5c34bb26852f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Values",
   "id": "e52c59d174d06d01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_episode_stats(stats, smoothing_window=10, no_show=False):\n",
    "    \"\"\"Function to plot the Episode statistics\n",
    "    Adapted from: https://github.com/dennybritz/reinforcement-learning/blob/master/lib/plotting.py\"\"\"\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(stats.episode_lengths)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Length\")\n",
    "    plt.title(\"Episode Length over Time\")\n",
    "    if no_show:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "\n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=(10, 5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    if no_show:\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show(fig2)\n",
    "\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    if no_show:\n",
    "        plt.close(fig3)\n",
    "    else:\n",
    "        plt.show(fig3)\n",
    "\n",
    "    return fig1, fig2, fig3"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_values(V, shape):\n",
    "    \"\"\"Plot State Values\"\"\"\n",
    "    # Reshape the value function V to the given shape\n",
    "    V = np.reshape(V, shape)\n",
    "\n",
    "    # Create a new figure with a specific size\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Add a subplot to the figure. 111 means 1x1 grid, first subplot.\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Display the reshaped value function V as an image on the axes\n",
    "    im = ax.imshow(V, cmap='cool')\n",
    "\n",
    "    # For each value in V, annotate the plot with the value's rounded label\n",
    "    for (j, i), label in np.ndenumerate(V):\n",
    "        ax.text(i, j, np.round(label, 3), ha='center', va='center', fontsize=14)\n",
    "\n",
    "    # Disable ticks and tick labels for both x and y axes\n",
    "    plt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n",
    "\n",
    "    # Set the title for the plot\n",
    "    plt.title('State-Value Function')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ],
   "id": "2ff6abc4f442035c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simulate and show video",
   "id": "62a4dc75ace8362b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def simulate_and_show_video(env_id, Q, epsilon):\n",
    "    \"\"\"\n",
    "    Simulate an environment using a given policy and display the video.\n",
    "\n",
    "    Parameters:\n",
    "    - env_id: The string ID of the environment to simulate.\n",
    "    - Q: The action-value function (Q-table).\n",
    "    - epsilon: The epsilon value for the epsilon-greedy policy.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - HTML object if video was captured, or a message indicating no videos were found.\n",
    "    \"\"\"\n",
    "    video_folder = f\"./{env_id}_videos\"\n",
    "\n",
    "    # Create the environment with the specified render_mode and wrap it\n",
    "    # The RecordVideo wrapper handles the video creation.\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env = RecordVideo(env, video_folder=video_folder)\n",
    "\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon, env)  # Use epsilon_greedy_policy to select action\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # Close the environment, which finalizes the video recording.\n",
    "    env.close()\n",
    "\n",
    "    # Look for the '.mp4' files in the correct directory\n",
    "    mp4list = glob.glob(f'{video_folder}/*.mp4')\n",
    "\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        # Clean up the directory after displaying the video\n",
    "        for file in mp4list:\n",
    "            os.remove(file)\n",
    "        os.rmdir(video_folder)\n",
    "\n",
    "        return HTML(data=f'''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
    "                        <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\" />\n",
    "                      </video>''')\n",
    "    else:\n",
    "        return \"No videos found or error during video creation.\""
   ],
   "id": "373c83de7a516ff1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def simulate_and_show_video_with_policy(env_id, policy):\n",
    "    \"\"\"\n",
    "    Simulate an environment using a given policy and display the video.\n",
    "\n",
    "    Parameters:\n",
    "    - env_id: The string ID of the environment to simulate.\n",
    "    - policy: The policy to use for action selection.\n",
    "\n",
    "    Returns:\n",
    "    - HTML object if video was captured, or a message indicating no videos were found.\n",
    "    \"\"\"\n",
    "    video_folder = f\"./{env_id}_videos_policy\" # Use a different folder name to avoid conflicts\n",
    "\n",
    "    # Create the environment with the specified render_mode and wrap it\n",
    "    # The RecordVideo wrapper handles the video creation.\n",
    "    env = gym.make(env_id, render_mode='rgb_array')\n",
    "    env = RecordVideo(env, video_folder=video_folder)\n",
    "\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = policy[state] # Use the policy to select action\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    # Close the environment, which finalizes the video recording.\n",
    "    env.close()\n",
    "\n",
    "    # Look for the '.mp4' files in the correct directory\n",
    "    mp4list = glob.glob(f'{video_folder}/*.mp4')\n",
    "\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        # Clean up the directory after displaying the video\n",
    "        for file in mp4list:\n",
    "            os.remove(file)\n",
    "        os.rmdir(video_folder)\n",
    "\n",
    "\n",
    "        return HTML(data=f'''<video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
    "                        <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\" />\n",
    "                      </video>''')\n",
    "    else:\n",
    "        return \"No videos found or error during video creation.\""
   ],
   "id": "47d28f1f753de718"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Q-Learning",
   "id": "6a12d425d7453ca9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helper Functions",
   "id": "a3c7dc4d87e3cfda"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Epsilon Greedy Action Selection",
   "id": "341b858de7a58880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon, env):\n",
    "    \"\"\"\n",
    "    Select an action using epsilon-greedy policy.\n",
    "\n",
    "    Parameters:\n",
    "    - Q 2D np array: A 2S array that maps state to action values.\n",
    "                For example, Q[state] = [0.1, 0.2, 0.4] for a 3-action environment.\n",
    "    - state (int): Current state of the agent in the environment.\n",
    "\n",
    "    - epsilon (float): The probability of choosing a random action (exploration factor).\n",
    "                   It should be between 0 (no exploration) and 1 (only exploration).\n",
    "\n",
    "    - env: The environment to sample random actions from.\n",
    "\n",
    "    Returns:\n",
    "    - action (int): The chosen action based on the epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    # Generate a random number between 0 and 1. If it's less than epsilon,\n",
    "    # then choose a random action, else the greedy action (the action with the maximum Q-value for the current state)\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return max(list(range(env.action_space.n)), key=lambda x: Q[state, x])"
   ],
   "id": "6bd07f09ea6df962"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Policy Extraction from Optimal Q-Values",
   "id": "cd3f155c8c8779e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_policy_V_from_Q_flat(Q, env):\n",
    "    \"\"\"\n",
    "    Extract flat policy and state-value function V from Q.\n",
    "\n",
    "    Params:\n",
    "    - Q: Action-value function.\n",
    "    - env: The environment to extract policy and V for.\n",
    "\n",
    "    Returns:\n",
    "    - policy: Derived flat policy from Q.\n",
    "    - V: Derived flat state-value function from Q.\n",
    "    \"\"\"\n",
    "\n",
    "    n_states = env.observation_space.n\n",
    "    policy = np.empty(n_states, dtype=np.int64)\n",
    "    V = np.empty(n_states, dtype=np.float64)\n",
    "\n",
    "    for state in range(n_states):\n",
    "            policy[state] = np.argmax(Q[state, :])\n",
    "            V[state] = np.max(Q[state, :])\n",
    "\n",
    "    return policy, V"
   ],
   "id": "53e43ff90332c9b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Q-Learning Algorithm",
   "id": "6aa97f01c6de2973"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def q_learning(env, num_episodes, gamma=1.0, alpha=0.85, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        gamma: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        Q is the optimal action-value function,\n",
    "        stats is a named tuple, which returns the episode statistics for plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Q values/Table to Zero\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n), dtype=float)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Run through several episodes\n",
    "    for episode in range(num_episodes):\n",
    "        # Store cumulative reward of each episodes in r\n",
    "        r = 0\n",
    "\n",
    "        # initialize the state,\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Reset the Episodes time step, will be used for stats\n",
    "        timeStep = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # select the action using epsilon-greedy policy\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon, env)\n",
    "\n",
    "            # then perform the action and move to the next state, and receive the reward\n",
    "            nextstate, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[episode] += reward\n",
    "            timeStep += 1\n",
    "            stats.episode_lengths[episode] = timeStep\n",
    "\n",
    "            # Calculate the Q-value of previous state using our update rule\n",
    "            # Q[(state,action)] += alpha * (reward + gamma * Q[(nextstate,nextBestaction)]-Q[(state,action)])\n",
    "            # Now, select the next BEST ACTION using epsilon greedy policy\n",
    "            best_next_action = np.argmax(Q[nextstate, :])\n",
    "            TD_target = reward + gamma * Q[nextstate, best_next_action]\n",
    "            TD_error = TD_target - Q[state, action]\n",
    "\n",
    "            Q[state, action] += alpha * TD_error\n",
    "\n",
    "            # Update the state and BUT NOT action with next action and next state\n",
    "            state = nextstate\n",
    "\n",
    "            # store the rewards\n",
    "            r += reward\n",
    "\n",
    "            # Break the loop, if it is the terminal state of the episode\n",
    "            if done:\n",
    "                break\n",
    "        # print(\"Total Reward : \", r)\n",
    "    return Q, stats"
   ],
   "id": "54d556f7f4103ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Running Q-Learning on CliffWalking Environment",
   "id": "bb3bf032d6360225"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### CliffWalking Environment Exploration\n",
    "\n",
    "The CliffWalking environment is a standard undiscounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. The reward is -1 on all transitions except those into the region marked \"The Cliff\". Stepping into this region incurs a reward of -100 and sends the agent instantly back to the start."
   ],
   "id": "4bf6bd678d88bc76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create an instance of the 'CliffWalking-v1' environment.\n",
    "# 'CliffWalking' is a standard gridworld environment where the agent has to navigate from a start state to a goal state,\n",
    "# avoiding falling off a cliff.\n",
    "env = gym.make('CliffWalking-v1')\n",
    "\n",
    "# Print the action space of the environment.\n",
    "# This will show the number of possible actions an agent can take in the environment.\n",
    "print(env.action_space)\n",
    "\n",
    "# Print the observation space of the environment.\n",
    "# This will show the number of possible states (or observations) an agent can encounter in the environment.\n",
    "print(env.observation_space)"
   ],
   "id": "4380d63bc25ffe5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Training the Agent using Q-Learning",
   "id": "131d1a6345be70c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train agent using the Q-Learning algorithm\n",
    "Q_table, stats = q_learning(env, 5000, gamma=1.0, alpha=0.5, epsilon=0.1)"
   ],
   "id": "9d3e954211df1f7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Extract the Optimal Policy and Visualize the Results",
   "id": "ad49b8f86a3c1eb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The Episode Statistics\n",
    "plot_episode_stats(stats)"
   ],
   "id": "2401dfdce8d0708d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract the optimal policy and corresponding state-value function (V)\n",
    "# from the action-value function (Q_table) using the 'extract_policy_V_from_Q_flat' function.\n",
    "# 'Q_table' is the action-value function derived from the Q-Learning algorithm.\n",
    "policy_QL, V_QL = extract_policy_V_from_Q_flat(Q_table, env)\n",
    "\n",
    "# Visualize the state-value function (V_table) using the 'plot_values' function.\n",
    "# The state-value function is reshaped into a grid of size (4, 12) for visualization.\n",
    "# In the context of the 'CliffWalking-v0' environment, the grid represents the layout of the environment\n",
    "# with 4 rows and 12 columns.\n",
    "plot_values(V_QL, (4, 12))"
   ],
   "id": "d03e6b8655ea1ca2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Simulate the optimal policy and display the video",
   "id": "69cd0c5eaa9646a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Simulate the environment using the optimal policy suggested by Q-Learning and display the video\n",
    "simulate_and_show_video('CliffWalking-v1', Q_table, epsilon=0.1)"
   ],
   "id": "b930d57001434377"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
